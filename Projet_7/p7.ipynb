{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJET N°7 - Réalisez des indexations automatiques d’images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux approches sont présentées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une approche classique : il s’agit de pre-processer des images avec des techniques spécifiques (e.g. whitening, equalisation, filtre linéaire/laplacien/gaussien, éventuellement modifier la taille des images), puis d’extraire des features (e.g. texture, corners, edges et SIFT detector). \n",
    "\n",
    "Il faut ensuite réduire les dimensions, soit par des approches classiques (e.g. PCA, k-means) soit avec une approche par histogrammes et dictionary learning (bag-of-words appliqué aux images), puis appliquer des algorithmes de classification standards.\n",
    "\n",
    "Lors de l’analyse exploratoire, vous regarderez si les features extraites et utilisées en classification sont prometteuses en utilisant des méthodes de réduction de dimension pour visualiser le dataset en 2D. Cela vous permettra d’affiner votre intuition sur les différents traitements possibles, sans que cela ne se substitue à des mesures de performances rigoureuses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une approche s’appuyant sur l’état de l’art et l’utilisation de CNN (réseaux de neurones convolutionnels). Compte tenu de la taille et de la complexité du dataset, et de la puissance de calcul à votre disposition, il est très difficile d’obtenir de bonnes performances. \n",
    "\n",
    "Aussi, est-il recommandé d’utiliser le transfer learning, c’est-à-dire utiliser un réseau déjà entraîné, et le modifier pour répondre à votre problème. Une première chose obligatoire est de ré-entraîner les dernières couches pour prédire les classes qui vous intéressent seulement. \n",
    "\n",
    "Il est également possible d’adapter la structure (supprimer certaines couches par exemple) ou de ré-entraîner le modèle avec un très faible learning rate pour ajuster les poids à votre problème (plus long) et optimiser les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries nécessaires au projet\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.ndimage\n",
    "from scipy.cluster.vq import whiten\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import cv2\n",
    "\n",
    "# Pour ne pas avoir les warnings lors de la compilation\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lieu où se trouvent des images\n",
    "DOSSIER_SOURCE = '/home/toni/Bureau/p7/flow/'\n",
    "IMG_DIR = '/home/toni/Bureau/p7/Images/'\n",
    "SAVE_LIST = '/home/toni/Bureau/'\n",
    "\n",
    "# Définitions des limites d'execution\n",
    "NB_RACES = 5\n",
    "NB_EXEMPLES = 200\n",
    "NB_CLUSTER = int(NB_RACES * (NB_EXEMPLES/5))\n",
    "AFFICHAGE_HISTOGRAMME = True\n",
    "T_IMG = 224\n",
    "BATCH_SIZE = 32\n",
    "DATA_AUGMENTATION = False\n",
    "\n",
    "RESULTATS = pd.DataFrame()\n",
    "\n",
    "# Setup a standard image size\n",
    "STANDARD_SIZE = (300, 167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Gestion de l'erreur quand une catégorie de chien n'est pas prédite. On rajoute la colonne vide manuellement</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gestion_erreur(res, test_y, labels, classifieur):\n",
    "\n",
    "    # Si ce n'est pas un kmeans, le traitement est différent (noms ou numéros)\n",
    "    if classifieur == 'kmeans':\n",
    "        for i in range(0, NB_RACES):\n",
    "            if i not in res.columns:\n",
    "                res[i] = 0\n",
    "\n",
    "        for i in np.unique(labels):\n",
    "            if i not in res.index:\n",
    "                res.loc[i] = 0\n",
    "\n",
    "    elif classifieur == 'cnn':\n",
    "        for i in res.index:\n",
    "            if i not in res.columns:\n",
    "                res[i] = 0\n",
    "    else:\n",
    "        for i in np.unique(test_y):\n",
    "            if i not in res.columns:\n",
    "                res[i] = 0\n",
    "\n",
    "        for i in res.columns:\n",
    "            if i not in res.index:\n",
    "                res.loc[i] = 0\n",
    "\n",
    "    res = res.sort_index(axis=0, ascending=True)\n",
    "    res = res.sort_index(axis=1, ascending=True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Les deux filtres qui sont testés</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonction_median(img, param1):\n",
    "    \"\"\"\n",
    "    Fonction de filtre\n",
    "    \"\"\"\n",
    "\n",
    "    # Application du filtre\n",
    "    img_modified = scipy.ndimage.median_filter(img, size=param1)\n",
    "\n",
    "    return img_modified\n",
    "\n",
    "def fonction_gauss(img, param1):\n",
    "    \"\"\"\n",
    "    Fonction de filtre\n",
    "    \"\"\"\n",
    "\n",
    "    # Application du filtre\n",
    "    img_modified = scipy.ndimage.filters.gaussian_filter(img, sigma=param1)\n",
    "\n",
    "    return img_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Transformation d'une image dans un array Numpy</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_matrix(filename, verbose=False):\n",
    "\n",
    "    img = Image.open(filename)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"changing size from %s to %s\" % (str(img.size), str(STANDARD_SIZE)))\n",
    "\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "    img = list(img.getdata())\n",
    "    img = np.array(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Transformation d'un array Numpy en un array à 1 dimension</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_image(img):\n",
    "\n",
    "    shape = img.shape[0] * img.shape[1]\n",
    "    img_wide = img.reshape(1, shape)\n",
    "    return img_wide[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Fonction qui récupére toute les images avec une sélection aléatoire. Rajout de filtres possibles</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recup_images_filtres(liste_images, num_filtre):\n",
    "\n",
    "    # Création des listes vides\n",
    "    data = []\n",
    "\n",
    "    for lien_image in liste_images:\n",
    "        # Récupération de la matrice tranformée\n",
    "        img = img_to_matrix(lien_image, False)\n",
    "\n",
    "        if num_filtre == 1:\n",
    "            # Filtre gaussien\n",
    "            img = fonction_gauss(img, 5)\n",
    "        elif num_filtre == 2:\n",
    "            # Filtre médian\n",
    "            img = fonction_median(img, 5)\n",
    "        elif num_filtre == 3:\n",
    "            img = whiten(img)\n",
    "\n",
    "        # Mise à une dimension\n",
    "        img = flatten_image(img)\n",
    "        data.append(img)\n",
    "\n",
    "        del img\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Detection et extraction des features d'une image</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(img, extractor):\n",
    "\n",
    "    img = cv2.imread(img)\n",
    "    img = cv2.resize(img, STANDARD_SIZE)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints, des = extractor.detectAndCompute(img, None)\n",
    "\n",
    "    return keypoints, des"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Fonction qui va calculer les pourcentages de bons pronostics.</b></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_resultats(res, test_y, classifieur, nom_filtre):\n",
    "\n",
    "    global RESULTATS\n",
    "        \n",
    "    print(\"\\nResultats pour\", classifieur)\n",
    "\n",
    "    # Transformation en tableau exploitable\n",
    "    res1 = res.values\n",
    "\n",
    "    data_resultats = pd.DataFrame(index=res.index, columns=['bons',\n",
    "                                                            'prono',\n",
    "                                                            'total',\n",
    "                                                            'pc_prono',\n",
    "                                                            'pc_total'])\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Resultat :\", round(100*res1.diagonal().sum()/len(test_y), 2), \"%\")\n",
    "    print(\"No d'erreurs =\", len(test_y) - res1.diagonal().sum(), \"/\", len(test_y))\n",
    "\n",
    "    for i in range(0, len(res)):\n",
    "        diagonale = res1.diagonal()[i]\n",
    "        data_resultats.loc[res.index[i], 'bons'] = diagonale\n",
    "        data_resultats.loc[res.index[i], 'prono'] = res.sum()[i]\n",
    "        data_resultats.loc[res.index[i], 'total'] = res.sum('columns')[i]\n",
    "        data_resultats.loc[res.index[i], 'pc_prono'] = round(100*diagonale/res.sum()[i], 2)\n",
    "        data_resultats.loc[res.index[i], 'pc_total'] = round(100*diagonale/res.sum('columns')[i], 2)\n",
    "\n",
    "    data_resultats = data_resultats.fillna(0)\n",
    "    \n",
    "    temp = []\n",
    "    temp.append([classifieur,\n",
    "                 nom_filtre,\n",
    "                 data_resultats['pc_prono'].mean(),\n",
    "                 data_resultats['pc_total'].mean()])\n",
    "\n",
    "    RESULTATS = RESULTATS.append(temp)\n",
    "    \n",
    "    print(data_resultats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Fonction qui extrait les features et permets de les clusterizer.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonction_orb(liste_images):\n",
    "\n",
    "    # Création des listes vides\n",
    "    pool_descriptors = []\n",
    "\n",
    "    #\n",
    "    extractor = cv2.ORB_create()\n",
    "\n",
    "    for lien_image in liste_images:\n",
    "        # Récupération de la matrice tranformée\n",
    "        keypoints, descriptors = features(lien_image, extractor)\n",
    "\n",
    "        # Rajout à la liste\n",
    "        pool_descriptors.append(descriptors)\n",
    "\n",
    "    # Mise au bon format\n",
    "    pool_descriptors = np.asarray(pool_descriptors)\n",
    "    pool_descriptors = np.concatenate(pool_descriptors, axis=0)\n",
    "\n",
    "    # Clusturisation des descriptors\n",
    "    print(\"Training MiniBatchKMeans\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=NB_CLUSTER).fit(pool_descriptors)\n",
    "    print(\"End training MiniBatchKMeans\")\n",
    "\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Création de la liste aléatoire des chiens pour les races selectionnés.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etablir_liste_chiens():\n",
    "\n",
    "    # Listes\n",
    "    liste_dossier = []\n",
    "    liste_images = []\n",
    "    liste_images_cnn = []\n",
    "    labels = []\n",
    "\n",
    "    # Valeur initiale d'un compteur\n",
    "    cpt_race = 0\n",
    "\n",
    "    # Création de la liste aléatoire des races\n",
    "    liste_chiens = os.listdir(IMG_DIR)\n",
    "    for i in range(0, NB_RACES):\n",
    "        nb_alea = random.randrange(0, len(liste_chiens))\n",
    "        liste_dossier.append(liste_chiens[nb_alea])\n",
    "        del liste_chiens[nb_alea]\n",
    "\n",
    "    # Création de la liste aléatoire des chiens pour les races selectionnés\n",
    "    for dirs in liste_dossier:\n",
    "        # Valeur initiale d'un compteur\n",
    "        cpt_exemple = 0\n",
    "        if cpt_race < NB_RACES+1:\n",
    "            cpt_race = cpt_race+1\n",
    "            for filename in os.listdir(IMG_DIR + dirs):\n",
    "                # On ne garde que NB_EXEMPLES exemplaires de chaque race\n",
    "                if cpt_exemple < NB_EXEMPLES:\n",
    "                    cpt_exemple = cpt_exemple+1\n",
    "\n",
    "                    # Chemin complet de l'image\n",
    "                    liste_images.append(IMG_DIR + dirs + '/' + filename)\n",
    "                    liste_images_cnn.append(filename)\n",
    "\n",
    "                    # Rajout du label\n",
    "                    labels.append(dirs[dirs.find('-')+1:].lower())\n",
    "\n",
    "    # Transformation de dataframe et exportation en csv\n",
    "    liste_images_cnn = pd.DataFrame(liste_images_cnn, columns=['liste'])\n",
    "    liste_images_cnn['labels'] = labels\n",
    "    liste_images_cnn.to_csv(SAVE_LIST + 'liste.csv')\n",
    "    \n",
    "    return liste_images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Création des centroids par la création des histogrammes des keypoints/features du kmeans.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centroids_histogram(liste_images, labels, model):\n",
    "\n",
    "    # Création des listes vides\n",
    "    feature_vectors = []\n",
    "    class_vectors = []\n",
    "    compteur = 0\n",
    "\n",
    "    # Extracteur de features\n",
    "    extractor = cv2.ORB_create() #cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    for lien_image in liste_images:\n",
    "        # Récupération de la matrice tranformée\n",
    "        keypoints, descriptors = features(lien_image, extractor)\n",
    "\n",
    "        # classification of all descriptors in the model\n",
    "        predict_kmeans = model.predict(descriptors)\n",
    "\n",
    "        # calculates the histogram\n",
    "        hist, bin_edges = np.histogram(predict_kmeans, bins=NB_CLUSTER)\n",
    "\n",
    "        # Affichage des histogrammes\n",
    "        if AFFICHAGE_HISTOGRAMME and compteur < 6:\n",
    "            compteur = compteur + 1\n",
    "            plt.hist(hist, bins=len(bin_edges), align='mid')\n",
    "            plt.xlabel('bins')\n",
    "            plt.ylabel('valeurs')\n",
    "            plt.title('Histogramme')\n",
    "            plt.show()\n",
    "\n",
    "        # histogram is the feature vector\n",
    "        feature_vectors.append(hist)\n",
    "\n",
    "    # Mise sous la bonne forme\n",
    "    feature_vectors = np.asarray(feature_vectors)\n",
    "    class_vectors = np.asarray(labels)\n",
    "\n",
    "    # return vectors and classes we want to classify\n",
    "    return class_vectors, feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Fonction avec utilise la technique bag of visual words.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonction_bovw(liste_images, labels):\n",
    "\n",
    "    print(\"\\nFiltre ORB\")\n",
    "\n",
    "    # Séparation des datasets testing/training\n",
    "    train_x, test_x, train_y, test_y = train_test_split(liste_images,\n",
    "                                                        labels,\n",
    "                                                        test_size=0.25)\n",
    "\n",
    "    # Entrainement du modèle sur le dataset de training\n",
    "    trained_model = fonction_orb(train_x)\n",
    "\n",
    "    # Extraction des histogrammes\n",
    "    [train_class, train_featvec] = calculate_centroids_histogram(train_x,\n",
    "                                                                 train_y,\n",
    "                                                                 trained_model)\n",
    "    [test_class, test_featvec] = calculate_centroids_histogram(test_x,\n",
    "                                                               test_y,\n",
    "                                                               trained_model)\n",
    "\n",
    "    # Utilisation des vecteurs de training pour entrainer le classifieur\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(train_featvec, train_class)\n",
    "    predict = clf.predict(test_featvec)\n",
    "\n",
    "    # Calcul des résultats\n",
    "    res = pd.crosstab(np.asarray(test_class),\n",
    "                      predict,\n",
    "                      rownames=[\"Actual\"],\n",
    "                      colnames=[\"Predicted\"])\n",
    "\n",
    "    # Gestion d'une erreur\n",
    "    if len(res.columns) != NB_RACES:\n",
    "        res = gestion_erreur(res, test_y, labels, 'svm')\n",
    "    calcul_resultats(res, test_y, 'svm', 'orb')\n",
    "\n",
    "    # Test avec KNN()\n",
    "    knn = KNeighborsClassifier(n_neighbors=50)\n",
    "    knn.fit(train_featvec, train_class)\n",
    "    predict = knn.predict(test_featvec)\n",
    "\n",
    "    # Calcul des résultats\n",
    "    res = pd.crosstab(np.asarray(test_class),\n",
    "                      predict,\n",
    "                      rownames=[\"Actual\"],\n",
    "                      colnames=[\"Predicted\"])\n",
    "\n",
    "    # Gestion d'une erreur\n",
    "    if len(res.columns) != NB_RACES:\n",
    "        res = gestion_erreur(res, test_y, labels, 'knn')\n",
    "    calcul_resultats(res, test_y, 'knn', 'orb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Fonction avec filtres traditionnels.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonction_filtres(liste_images, labels):\n",
    "\n",
    "    for num_filtre in range(0, 4):\n",
    "        if num_filtre == 0:\n",
    "            nom_filtre = \"Aucun\"\n",
    "        elif num_filtre == 1:\n",
    "            nom_filtre = \"Gaussien\"\n",
    "        elif num_filtre == 2:\n",
    "            nom_filtre = \"Median\"\n",
    "        elif num_filtre == 3:\n",
    "            nom_filtre = \"Whitening\"\n",
    "\n",
    "        print(\"\\nFiltre\", nom_filtre)\n",
    "        data = recup_images_filtres(liste_images, num_filtre)\n",
    "\n",
    "        ## Réduction de dimension\n",
    "        # PCA\n",
    "        pca = RandomizedPCA(n_components=2)\n",
    "        data = pca.fit_transform(data)\n",
    "        # Explication de la variance\n",
    "        #print(pca.explained_variance_ratio_)\n",
    "\n",
    "        # t-SNE\n",
    "        #data = TSNE(n_components=2).fit_transform(data, labels)\n",
    "\n",
    "        # Affichage en 2D après une décomposition\n",
    "        affichage_decomposition(data, labels)\n",
    "\n",
    "        # Séparation des datasets testing/training\n",
    "        train_x, test_x, train_y, test_y = train_test_split(data,\n",
    "                                                            labels,\n",
    "                                                            test_size=0.25)\n",
    "\n",
    "        # Transformation en array\n",
    "        test_y = np.array(test_y)\n",
    "        train_y = np.array(train_y)\n",
    "\n",
    "        ## Création de la méthode de classification\n",
    "        # Test avec KNN\n",
    "        knn = KNeighborsClassifier(n_neighbors=10)\n",
    "        knn.fit(train_x, train_y)\n",
    "        res = pd.crosstab(test_y,\n",
    "                          knn.predict(test_x),\n",
    "                          rownames=[\"Actual\"],\n",
    "                          colnames=[\"Predicted\"])\n",
    "\n",
    "        # Gestion d'une erreur\n",
    "        if len(res.columns) != NB_RACES:\n",
    "            res = gestion_erreur(res, test_y, '0', 'knn')\n",
    "        calcul_resultats(res, test_y, 'knn', nom_filtre)\n",
    "\n",
    "        # Test avec Kmeans\n",
    "        kmeans = KMeans(n_clusters=NB_RACES).fit(train_x, train_y)\n",
    "        res = pd.crosstab(test_y,\n",
    "                          kmeans.predict(test_x),\n",
    "                          rownames=[\"Actual\"],\n",
    "                          colnames=[\"Predicted\"])\n",
    "\n",
    "        # Gestion d'une erreur\n",
    "        if len(res.columns) != NB_RACES:\n",
    "            res = gestion_erreur(res, test_y, labels, 'kmeans')\n",
    "        calcul_resultats(res, test_y, 'kmeans', nom_filtre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Affichage en 2D de la décomposition.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affichage_decomposition(data, labels):\n",
    "\n",
    "    principaldf = pd.DataFrame(data=data,\n",
    "                             columns=['principal component 1',\n",
    "                                      'principal component 2'])\n",
    "\n",
    "    finaldf = pd.concat([principaldf, pd.DataFrame(labels)], axis=1)\n",
    "\n",
    "    data_labels = pd.DataFrame(labels)\n",
    "    targets = []\n",
    "\n",
    "    # Création de la liste des labels\n",
    "    for i in data_labels[0].unique():\n",
    "        targets.append(i)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    axe = fig.add_subplot(1, 1, 1)\n",
    "    axe.set_xlabel('Principal Component 1', fontsize=15)\n",
    "    axe.set_ylabel('Principal Component 2', fontsize=15)\n",
    "    axe.set_title('PCA components', fontsize=20)\n",
    "\n",
    "    # Création de la liste des couleurs\n",
    "    colors = matplotlib.cm.rainbow(np.linspace(0, 1, len(targets)))\n",
    "\n",
    "    # Affichage par catégorie\n",
    "    for target, color in zip(targets, colors):\n",
    "        indices_to_keep = finaldf[0] == target\n",
    "        axe.scatter(finaldf.loc[indices_to_keep, 'principal component 1'],\n",
    "                    finaldf.loc[indices_to_keep, 'principal component 2'],\n",
    "                    c=color,\n",
    "                    s=50)\n",
    "\n",
    "    axe.legend(targets)\n",
    "    axe.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions utiles pour le cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Fonction pour la data augmentation.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_augmentation(model, liste_train, liste_test):\n",
    "\n",
    "    # DA pour le training\n",
    "    train_datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
    "                                                                 width_shift_range=0.2,\n",
    "                                                                 height_shift_range=0.2,\n",
    "                                                                 shear_range=0.2,\n",
    "                                                                 zoom_range=0.2,\n",
    "                                                                 horizontal_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(dataframe=liste_train,\n",
    "                                                        directory=DOSSIER_SOURCE,\n",
    "                                                        x_col='liste',\n",
    "                                                        y_col='labels',\n",
    "                                                        has_ext=True,\n",
    "                                                        target_size=(T_IMG, T_IMG),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "    # DA pour la validation\n",
    "    valid_datagen = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "    valid_generator = valid_datagen.flow_from_dataframe(dataframe=liste_test,\n",
    "                                                        directory=DOSSIER_SOURCE,\n",
    "                                                        x_col='liste',\n",
    "                                                        y_col='labels',\n",
    "                                                        has_ext=True,\n",
    "                                                        target_size=(T_IMG, T_IMG),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "    hist = model.fit_generator(train_generator,\n",
    "                               steps_per_epoch=2000 // BATCH_SIZE,\n",
    "                               epochs=25,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=200 // BATCH_SIZE)\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Fonction de transfert learning.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_appel_vgg(x_train, y_train, x_valid, y_valid,liste_train, liste_test):\n",
    "\n",
    "    # On crée un modèle déjà pré entrainé\n",
    "    base_model = VGG19(weights=\"imagenet\", include_top=False, input_shape=(T_IMG, T_IMG, 3))\n",
    "\n",
    "    # On rajoute les deux dernières couches qui nous intéressent\n",
    "    x_model = base_model.output\n",
    "    x_model = Flatten()(x_model)\n",
    "    x_model = Dense(NB_RACES, activation='softmax')(x_model)\n",
    "\n",
    "    # On crée notre modèle à partir de celui existant, et des deux couches en plus\n",
    "    model = Model(inputs=base_model.input, outputs=x_model)\n",
    "\n",
    "    # On choisi d'entrainer que nos couches rajoutées\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compilatioon du modèle\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # On se donne des tours sans évolution pour stopper le fit\n",
    "    callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                                                    patience=5,\n",
    "                                                    verbose=1)]\n",
    "\n",
    "    # Visualisation de toutes les couches du modèle\n",
    "    model.summary()\n",
    "\n",
    "    if DATA_AUGMENTATION is True:\n",
    "        res = cnn_data_augmentation(model, liste_train, liste_test)\n",
    "    else:\n",
    "        # Entrainement\n",
    "        res = model.fit(x_train,\n",
    "                        y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=(x_valid, y_valid),\n",
    "                        verbose=1)\n",
    "\n",
    "    # Tracé de courbes pour visualiser les résultats\n",
    "    cnn_courbes(res)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Tracé des courbes d'accuracy et de log loss.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_courbes(resultat):\n",
    "\n",
    "    # Loss Curves\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.plot(resultat.history['loss'], 'r', linewidth=3.0)\n",
    "    plt.plot(resultat.history['val_loss'], 'b', linewidth=3.0)\n",
    "    plt.legend(['Training loss', 'Validation Loss'], fontsize=18)\n",
    "    plt.xlabel('Epochs ', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.title('Loss Curves', fontsize=16)\n",
    "\n",
    "    # Accuracy Curves\n",
    "    plt.figure(figsize=[8, 6])\n",
    "    plt.plot(resultat.history['acc'], 'r', linewidth=3.0)\n",
    "    plt.plot(resultat.history['val_acc'], 'b', linewidth=3.0)\n",
    "    plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)\n",
    "    plt.xlabel('Epochs ', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.title('Accuracy Curves', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:#5F04B4'><b>Fonction de récupération des images.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_recup_images():\n",
    "\n",
    "    # Listes vides\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "\n",
    "    # Récupération de la liste des images\n",
    "    liste_images = pd.read_csv(SAVE_LIST + 'liste.csv')\n",
    "    del liste_images['Unnamed: 0']\n",
    "    \n",
    "    # Séparation des datasets testing/training\n",
    "    liste_train, liste_test = train_test_split(liste_images,\n",
    "                                               test_size=0.2)\n",
    "\n",
    "    liste_train = liste_train.reset_index(drop=\"True\")\n",
    "    liste_test = liste_test.reset_index(drop=\"True\")\n",
    "\n",
    "    # Préparation du one-hot encoding\n",
    "    targets_series = pd.Series(liste_train['labels'])\n",
    "    one_hot = pd.get_dummies(targets_series, sparse=True)\n",
    "    one_hot_labels = np.asarray(one_hot)\n",
    "    \n",
    "    # Récupération des images et des labels de training\n",
    "    i = 0\n",
    "    for file, dump in tqdm(liste_train.values):\n",
    "        # Lecture de l'image\n",
    "        img = cv2.imread(DOSSIER_SOURCE + file)\n",
    "\n",
    "        # Rajout des données dans la liste\n",
    "        x_train.append(cv2.resize(img, (T_IMG, T_IMG)))\n",
    "\n",
    "        # Race du chien\n",
    "        y_train.append(one_hot_labels[i])\n",
    "        i = i + 1\n",
    "\n",
    "    # Récupération des images de testing\n",
    "    for file in tqdm(liste_test['liste'].values):\n",
    "        # Lecture de l'image\n",
    "        img = cv2.imread(DOSSIER_SOURCE + file)\n",
    "\n",
    "        # Rajout des données dans la liste\n",
    "        x_test.append(cv2.resize(img, (T_IMG, T_IMG)))\n",
    "\n",
    "    return liste_train, liste_test, x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etablir la liste des chiens\n",
    "liste_images, labels = etablir_liste_chiens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtre Aucun\n"
     ]
    }
   ],
   "source": [
    "fonction_filtres(liste_images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonction_bovw(liste_images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction principale pour le cnn\n",
    "\n",
    "La liste des chiens a été récupérée un peu plus haut. Pour avoir des résultats comparables, on joue les tests sur le même dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du même dataset\n",
    "liste_train, liste_test, x_train, y_train, x_test = cnn_recup_images()\n",
    "    \n",
    "# Reformatage des listes pour le format de VGG19\n",
    "y_train_raw = np.array(y_train, np.uint8)\n",
    "x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "x_test = np.array(x_test, np.float32) / 255.\n",
    "\n",
    "# Vérification des données\n",
    "print(x_train_raw.shape)\n",
    "print(y_train_raw.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# Séparation des datasets training/validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_raw,\n",
    "                                                      y_train_raw,\n",
    "                                                      test_size=0.3,\n",
    "                                                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction de transfer learning\n",
    "model = cnn_appel_vgg(x_train,\n",
    "                      y_train,\n",
    "                      x_valid,\n",
    "                      y_valid,\n",
    "                      liste_train,\n",
    "                      liste_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des résultats\n",
    "predictions = model.predict(x_valid)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "truth = np.argmax(y_valid, axis=1)\n",
    "\n",
    "res = pd.crosstab(np.asarray(truth),\n",
    "                  predictions,\n",
    "                  rownames=[\"Actual\"],\n",
    "                  colnames=[\"Predicted\"])\n",
    "\n",
    "# Gestion d'une erreur\n",
    "if len(res.columns) != NB_RACES:\n",
    "    res = gestion_erreur(res, predictions, liste_train['labels'], 'cnn')\n",
    "calcul_resultats(res, np.asarray(predictions), 'cnn', ' ')\n",
    "\n",
    "errors = np.where(predictions != truth)[0]\n",
    "print(\"No of errors =\", len(errors), \"/\", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résultats globaux\n",
    "\n",
    "RESULTATS.columns= [\"nom\", \"nom2\", \"% prono\", \"% total\"]\n",
    "print(RESULTATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration des CPU/GPU, ne pas lancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'CPU': 8} ) \n",
    "#config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
